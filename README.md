# LLM (Repositorio en construcción ⛏️🛠️🔨)
Este repositorio contendrá los recursos que me sirvieron para aprender sobre LLM's

## Cursos

| Cursos | Link |
|:-|:-:|
| Stanford Lecture - CS324 | https://stanford-cs324.github.io/winter2022/lectures/ |
|  MIT - TinyML and Efficient Deep Learning Computing |  https://hanlab.mit.edu/courses/2023-fall-65940 |
|  Coursera - Generative AI with Large Language Models* |  https://www.coursera.org/learn/generative-ai-with-llms |
|  HuggingFace - A little guide to building Large Language Models in 2024  |  https://youtu.be/2-SPH9hIKT8?si=7wPqBf8t20aln0G7   |
|                      |            |


* `*Curso de pago`

## Andrej Karpathy 

| Títulos | Links |
|:-:|:-:|
| Let's build GPT: from scratch, in code, spelled out | https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1803s |
| State of GPT | https://www.youtube.com/watch?v=bZQun8Y4L2A |
| [1hr Talk] Intro to Large Language Models | https://www.youtube.com/watch?v=zjkBMFhNj_g&t=2101s |
| Let's build the GPT Tokenizer | https://youtu.be/zduSFxRajkE?si=63aOezV4VH_G5IcU |

## YouTube Channels

| Canales de YouTube | Links |
|:-:|:-:|
|    |    |

## Conferencias de los Artículos

Conferencias de algunos papers que he ido encontrando 👌🏼.

| Conferencias de los Papers | Links | Artículos | GitHub | HuggingFace Blog |
|:-:|:-:|:-:|:-:|:-:|
|  QLoRA: Efficient Finetuning of Quantized LLMs (Tim Dettmers)  |  https://youtu.be/fQirE9N5q_Y?si=ALeiLxiQpb3sdkbL  |  [![arxiv paper](https://img.shields.io/badge/arXiv-Paper-red)](https://arxiv.org/abs/2305.14314)  | https://github.com/artidoro/qlora |  [![hfpaper](https://img.shields.io/badge/🤗HugginngFace-Blog-yellow)](https://huggingface.co/blog/4bit-transformers-bitsandbytes)  |
| 8-bit Methods for Efficient Deep Learning with Tim Dettmers | https://youtu.be/jyOqtw4ry2w?si=Bwm3emBeMnzwln0Y |  |   |   |

## Transformers

A continuación, dejaré links de vídeos acerca del transformer para aprenderlo en detalle 🤯🤯💥💥

|  |  |
|:-:|:-:|
|    |  |

## Artículos 

| Papers | Link |
|:-|:-:|
| Transformer | [![arxiv paper](https://img.shields.io/badge/arXiv-Paper-red)](https://arxiv.org/abs/1706.03762) |
|  Decoder-Only |   |
| GPT |   |
| GPT-2 |  |
| GPT-3 |  |
|    |     |
